# -*- coding: utf-8 -*-
"""Email_spam_ham.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dEsQ9aypKN9uBu4Z_cNYFlp1atSIrV0T
"""

!pip install datasets transformers torch scikit-learn flask pyngrok huggingface_hub

from huggingface_hub import login

# Your Hugging Face write token
login("")

from huggingface_hub import whoami

print(whoami())

from datasets import load_dataset
import pandas as pd

# Step 1Ô∏è‚É£: Load the SMS Spam Collection dataset
# This dataset has ~5,574 SMS messages labeled as 'spam' or 'ham' (not spam)
dataset = load_dataset("sms_spam")

# Check the dataset structure
print("Dataset keys:", dataset.keys())  # Usually 'train' only for this dataset
print("Number of examples:", len(dataset['train']))

# Step 2Ô∏è‚É£: Explore some samples
print("\nSample data points:")
for i in range(3):
    print(f"Message {i+1}:")
    print("Text:", dataset['train'][i]['sms'])
    print("Label:", "Spam" if dataset['train'][i]['label']==1 else "Ham")
    print("-"*40)

# Step 3Ô∏è‚É£: Convert to pandas DataFrame for easier exploration
df = pd.DataFrame(dataset['train'])
df['label_name'] = df['label'].apply(lambda x: "Spam" if x == 1 else "Ham")
print("\nDataFrame head:")
print(df.head())

# Step 4Ô∏è‚É£: Check class distribution
print("\nClass distribution:")
print(df['label_name'].value_counts())

# Optional: Visualize with a bar plot
import matplotlib.pyplot as plt

plt.figure(figsize=(5,4))
df['label_name'].value_counts().plot(kind='bar', color=['green','red'])
plt.title("Spam vs Ham distribution")
plt.ylabel("Number of messages")
plt.show()

# Step 5Ô∏è‚É£: Split into train/test sets
# We will use 80% of data for training, 20% for testing
train_test = dataset['train'].train_test_split(
    test_size=0.2,
    seed=42,
    stratify_by_column="label"  # ensures same class ratio in both sets
)
train_dataset = train_test['train']
test_dataset = train_test['test']

print("Train/Test split sizes:")
print("Train:", len(train_dataset))
print("Test:", len(test_dataset))

def show_class_distribution(ds, name="Dataset"):
    df = pd.DataFrame(ds)
    class_counts = df['label'].value_counts()
    print(f"\n{name} class distribution:")
    for label, count in class_counts.items():
        label_name = "Ham" if label == 0 else "Spam"
        print(f"{label_name}: {count} ({count/len(ds)*100:.2f}%)")

show_class_distribution(train_dataset, "Train")
show_class_distribution(test_dataset, "Test")

"""With stratification, the class ratios are preserved in both train and test sets:

Train: 86.6% Ham, 13.4% Spam

Test: 86.6% Ham, 13.4% Spam

This ensures the model sees a representative sample of both classes during training and evaluation.



Why stratification matters: Without it, a random split could end up with very few Spam messages in the test set, making evaluation unreliable.

Class imbalance: Spam messages are much fewer than Ham messages (‚âà1:6 ratio). You can discuss strategies like weighting, oversampling, or using metrics like F1-score instead of just accuracy.
"""

# Step 6Ô∏è‚É£: Preview first training sample
print("\nFirst training sample:")
print(train_dataset[0])

from transformers import AutoTokenizer

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def tokenize_batch(batch):
    return tokenizer(batch["sms"], padding="max_length", truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize_batch, batched=True)
test_dataset = test_dataset.map(tokenize_batch, batched=True)

train_dataset = train_dataset.rename_column("label", "labels")
test_dataset = test_dataset.rename_column("label", "labels")

train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
test_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

print(train_dataset.column_names)

print(train_dataset.features)

"""input_ids:     [101, 4659, 1037, 2489, 4262, 2085, 999, 102, 0, 0, 0, 0]

attention_mask:[1,   1,   1,   1,   1,   1,   1,   1,   0, 0, 0, 0]
"""

!pip install evaluate

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
import evaluate

# Load model
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)

# Metric
metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# ‚úÖ Training arguments - OLD compatible (no evaluation_strategy)
training_args = TrainingArguments(
    output_dir="./results",
    save_steps=500,                    # save checkpoint every 500 steps
    logging_dir="./logs",
    logging_steps=100,                 # log every 100 steps
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    report_to=[]                       # disables wandb auto logging
)

# Trainer (shows tqdm automatically)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

# üöÄ Train with tqdm
trainer.train()

# ‚úÖ Evaluate manually at the end
metrics = trainer.evaluate()
print("Final evaluation metrics:", metrics)

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
import evaluate

model.push_to_hub("Email-Spam")
tokenizer.push_to_hub("Email-Spam")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# from flask import Flask, render_template, request
# from transformers import pipeline
# 
# app = Flask(__name__)
# 
# # Load pipeline from your fine-tuned model on Hugging Face
# spam_clf = pipeline("text-classification", model="Raja-Rajeswari-Javvadi/Email-Spam")
# 
# @app.route("/", methods=["GET", "POST"])
# def home():
#     result = ""
#     message = ""
#     if request.method == "POST":
#         message = request.form.get("message", "").strip()
#         if message:
#             pred = spam_clf(message)[0]
#             label = "üö® Spam ‚ùå" if pred['label'] == "LABEL_1" else "‚úÖ Not Spam"
#             result = f"{label} (Confidence: {pred['score']:.2f})"
#     return render_template("index.html", result=result, message=message)
# 
# if __name__ == "__main__":
#     app.run(host="0.0.0.0", port=8000)
#

!mkdir -p templates

!mkdir -p static

# Commented out IPython magic to ensure Python compatibility.
# %%writefile templates/index.html
# <!DOCTYPE html>
# <html>
# <head>
#     <title>üìß Spam Detector</title>
#     <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
# </head>
# <body>
#     <div class="container">
#         <h1>üìß Email / SMS Spam Detector</h1>
#         <form method="post">
#             <textarea name="message" rows="5" placeholder="Enter your message here...">{{ message }}</textarea>
#             <button type="submit">Check üöÄ</button>
#         </form>
# 
#         {% if result %}
#         <div class="result">
#             <h2>{{ result }}</h2>
#         </div>
#         {% endif %}
#     </div>
# </body>
# </html>
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile static/style.css
# body {
#     font-family: 'Segoe UI', sans-serif;
#     background: linear-gradient(135deg, #141E30, #243B55);
#     color: #fff;
#     display: flex;
#     justify-content: center;
#     align-items: center;
#     height: 100vh;
#     margin: 0;
# }
# 
# .container {
#     text-align: center;
#     width: 50%;
#     background: rgba(255, 255, 255, 0.1);
#     padding: 30px;
#     border-radius: 16px;
#     box-shadow: 0 4px 20px rgba(0,0,0,0.5);
# }
# 
# h1 {
#     margin-bottom: 20px;
#     color: #FFD700;
# }
# 
# textarea {
#     width: 90%;
#     padding: 12px;
#     border-radius: 10px;
#     border: none;
#     outline: none;
#     font-size: 16px;
#     margin-bottom: 15px;
# }
# 
# button {
#     background: #FFD700;
#     color: #000;
#     font-weight: bold;
#     padding: 12px 20px;
#     border-radius: 8px;
#     border: none;
#     cursor: pointer;
#     transition: background 0.3s ease-in-out;
# }
# 
# button:hover {
#     background: #FFA500;
# }
# 
# .result {
#     margin-top: 20px;
#     font-size: 20px;
#     font-weight: bold;
#     padding: 15px;
#     border-radius: 12px;
#     background: rgba(0, 0, 0, 0.4);
# }
#

# ‚úÖ Kill any running Flask/ngrok processes
!pkill -f flask || echo "No flask running"
!pkill -f ngrok || echo "No ngrok running"

# ‚úÖ Check if port 8000 is occupied
!lsof -i :8000 || echo "Port 8000 is free"

# (Optional) If any PID shows up in the above output, kill it:
!kill -9 23863

# ‚úÖ Run Flask in the background and log output
!nohup python app.py > flask.log 2>&1 &

!tail -n 50 flask.log

# ‚úÖ Start ngrok tunnel
from pyngrok import ngrok, conf
conf.get_default().auth_token = "345QO4rgr00ARzL1WTwD44ltomA_7zUWjh8GD1cJtLAhbqMuA"

public_url = ngrok.connect(8000)
print("üåç Public URL:", public_url)

# ‚úÖ Check Flask logs (useful if error happens)
!sleep 3 && tail -n 20 flask.log

